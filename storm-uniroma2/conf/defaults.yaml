# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


########### These all have default values as shown
########### Additional configuration goes into storm.yaml

java.library.path: "/usr/local/lib:/opt/local/lib:/usr/lib"

### storm.* configs are general configurations
# the local dir is where jars are kept
storm.local.dir: "storm-local"
storm.zookeeper.servers:
    - "localhost"
storm.zookeeper.port: 2181
storm.zookeeper.root: "/storm"
storm.zookeeper.session.timeout: 20000
storm.zookeeper.connection.timeout: 15000
storm.zookeeper.retry.times: 5
storm.zookeeper.retry.interval: 1000
storm.zookeeper.retry.intervalceiling.millis: 30000
storm.cluster.mode: "distributed" # can be distributed or local
storm.local.mode.zmq: false
storm.thrift.transport: "backtype.storm.security.auth.SimpleTransportPlugin"
storm.messaging.transport: "backtype.storm.messaging.netty.Context"

### nimbus.* configs are for the master
nimbus.host: "localhost"
nimbus.thrift.port: 6627
nimbus.thrift.max_buffer_size: 1048576
nimbus.childopts: "-Xmx1024m"
nimbus.task.timeout.secs: 30
nimbus.supervisor.timeout.secs: 60
nimbus.monitor.freq.secs: 10
nimbus.cleanup.inbox.freq.secs: 600
nimbus.inbox.jar.expiration.secs: 3600
nimbus.task.launch.secs: 120
nimbus.reassign: true
nimbus.file.copy.expiration.secs: 600
nimbus.topology.validator: "backtype.storm.nimbus.DefaultTopologyValidator"

### ui.* configs are for the master
ui.port: 8080
ui.childopts: "-Xmx768m"

logviewer.port: 8000
logviewer.childopts: "-Xmx128m"
logviewer.appender.name: "A1"


drpc.port: 3772
drpc.worker.threads: 64
drpc.queue.size: 128
drpc.invocations.port: 3773
firestorm.enabled: true
drpc.request.timeout.secs: 600
drpc.childopts: "-Xmx768m"

transactional.zookeeper.root: "/transactional"
transactional.zookeeper.servers: null
transactional.zookeeper.port: null

### supervisor.* configs are for node supervisors
# Define the amount of workers that can be run on this machine. Each worker is assigned a port to use for communication
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703
supervisor.childopts: "-Xmx256m"
#how long supervisor will wait to ensure that a worker process is started
supervisor.worker.start.timeout.secs: 120
supervisor.worker.start.retry.max: 5
#how long between heartbeats until supervisor considers that worker dead and tries to restart it
supervisor.worker.timeout.secs: 30
#how frequently the supervisor checks on the status of the processes it's monitoring and restarts if necessary
supervisor.monitor.frequency.secs: 3
#how frequently the supervisor heartbeats to the cluster state (for nimbus)
supervisor.heartbeat.frequency.secs: 5
supervisor.enable: true

### worker.* configs are for task workers
worker.childopts: "-Xmx768m"
worker.heartbeat.frequency.secs: 1

# control how many worker receiver threads we need per worker 
topology.worker.receiver.thread.count: 1

task.heartbeat.frequency.secs: 3
task.refresh.poll.secs: 1

zmq.threads: 1
zmq.linger.millis: 5000
zmq.hwm: 0


storm.messaging.netty.server_worker_threads: 1
storm.messaging.netty.client_worker_threads: 1
storm.messaging.netty.buffer_size: 5242880 #5MB buffer
storm.messaging.netty.max_retries: 30
storm.messaging.netty.max_wait_ms: 1000
storm.messaging.netty.min_wait_ms: 100

# If the Netty messaging layer is busy(netty internal buffer not writable), the Netty client will try to batch message as more as possible up to the size of storm.messaging.netty.transfer.batch.size bytes, otherwise it will try to flush message as soon as possible to reduce latency.
storm.messaging.netty.transfer.batch.size: 262144

# We check with this interval that whether the Netty channel is writable and try to write pending messages if it is.
storm.messaging.netty.flush.check.interval.ms: 10

### topology.* configs are for specific executing storms
topology.enable.message.timeouts: true
topology.debug: false
topology.workers: 1
topology.acker.executors: null
topology.tasks: null
# maximum amount of time a message has to complete before it's considered failed
topology.message.timeout.secs: 30
topology.multilang.serializer: "backtype.storm.multilang.JsonSerializer"
topology.skip.missing.kryo.registrations: false
topology.max.task.parallelism: null
topology.max.spout.pending: null
topology.state.synchronization.timeout.secs: 60
topology.stats.sample.rate: 0.05
topology.builtin.metrics.bucket.size.secs: 60
topology.fall.back.on.java.serialization: true
topology.worker.childopts: null
topology.executor.receive.buffer.size: 1024 #batched
topology.executor.send.buffer.size: 1024 #individual messages
topology.receiver.buffer.size: 8 # setting it too high causes a lot of problems (heartbeat thread gets starved, throughput plummets)
topology.transfer.buffer.size: 1024 # batched
topology.tick.tuple.freq.secs: null
topology.worker.shared.thread.pool.size: 4
topology.disruptor.wait.strategy: "com.lmax.disruptor.BlockingWaitStrategy"
topology.spout.wait.strategy: "backtype.storm.spout.SleepSpoutWaitStrategy"
topology.sleep.spout.wait.strategy.time.ms: 1
topology.error.throttle.interval.secs: 10
topology.max.error.report.per.interval: 5
topology.kryo.factory: "backtype.storm.serialization.DefaultKryoFactory"
topology.tuple.serializer: "backtype.storm.serialization.types.ListDelegateSerializer"
topology.trident.batch.emit.interval.millis: 500

dev.zookeeper.path: "/tmp/dev-storm-zookeeper"

### Adaptive Scheduler 
## Extension developed by the University of Rome "Tor Vergata"
## Author: Matteo Nardelli
adaptivescheduler.enabled: false

adaptivescheduler.initial_scheduler.location_aware: false
adaptivescheduler.just_monitor: false

adaptivescheduler.space.use_extended_space: false
adaptivescheduler.space.latency.max: 38.0
adaptivescheduler.space.weight.1: 1.0
adaptivescheduler.space.weight.2: 1.0
adaptivescheduler.space.weight.3: 1.0
adaptivescheduler.space.reliability: 90.0
adaptivescheduler.space.third.as.utilization: true

## Adaptive Scheduler: Advanced Parameters
## ## Network Space
adaptivescheduler.network_space.round.duration: 30
adaptivescheduler.network_space.alpha: 0.5
adaptivescheduler.network_space.beta: 0.5
adaptivescheduler.network_space.server.port: 9110
adaptivescheduler.network_space.confidence_threshold: 0.70
adaptivescheduler.network_space.round_between_publications: 1

## ## Scheduler
adaptivescheduler.continuous_scheduler.freq.sec: 30
adaptivescheduler.internal.database.port: 3603
adaptivescheduler.continuous_scheduler.force.threshold: 1.0
adaptivescheduler.continuous_scheduler.force.delta: 0.1
adaptivescheduler.continuous_scheduler.max_exec_per_slot: 4
adaptivescheduler.continuous_scheduler.nearest_node.k: 5
adaptivescheduler.continuous_scheduler.migration_threshold: 0.15

## ## Worker Monitor
adaptivescheduler.worker_monitor.enabled: true
adaptivescheduler.worker_monitor.stats.freq.sec: 5
### End of Adaptive Scheduler configuration section ###

### Adaptive Scheduler 
## Extension developed by the University of Rome "Tor Vergata"
## Author: Dario Luzi
taos.smooth.eof.timeout: 120000
taos.smooth.timeout: 180000
taos.queue_directory_check_period: 5000
taos.queue_delete_timeout: 120000
taos.cpu_time_advertise_period: 100
taos.metrics_smoothing: 0.5
taos.metrics_update_period: 10000
taos.db_ttl: 900
taos.mongo_db_ip: "localhost"
taos.mongo_db_port: 27017
taos.generator_ip: "localhost"
taos.generator_port: 8080
taos.target_utilization: 1.0
taos.schedule_fetch_time: 120000
taos.smooth.enabled: true
autoscaling.downscale_threshold: 0.2
autoscaling.upscale_threshold: 0.8
autoscaling.weak_scale_threshold: 0.5
autoscaling.max_downscale_factor: 0
autoscaling.scale_to_max_threshold: 0.10
metrics.util.norm: 0.75
metrics.spout.name: "input"

